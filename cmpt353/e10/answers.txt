1. How long did your reddit_averages.py take with (1) the reddit-0 data set and effectively no work, (2) no schema specified and not caching (on reddit-2 for this and the rest), (3) with a schema but not caching, (4) with both a schema and caching the twice-used DataFrame? [The reddit-0 test is effectively measuring the Spark startup time, so we can see how long it takes to do the actual work on reddit-2 in the best/worst cases.]

(1) 0m10.402s
(2) 1m27.328s
(3) 0m14.117s
(4) 0m11.467s


2. Based on the above, does it look like most of the time taken to process the reddit-2 data set is in reading the files, or calculating the averages?

Reading the files, since when there is no schema assigned, the runtime is very slow.

3. Where did you use .cache() in your wikipedia_popular.py? [Hint: the answer had better be “once”… but where?]

The cache happens in between "after I finished filtering the wiki_views" and "before I look for the max view count". So that after calculating the max, I can use the cached version of origianl data to join with the max.

